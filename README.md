# NormaCS Unified Feed (Starter)

Единая лента новостей из 18 источников. Сбор — каждый час на GitHub Actions, публикация — через GitHub Pages из папки `/docs` ветки `main`.

## Что будет в итоге
- Публичный URL ленты JSON: `https://<ваш-логин>.github.io/<repo>/unified.json`
- Простая страница: `https://<ваш-логин>.github.io/<repo>/`
- Лента содержит поля: `id`, `url`, `title`, `date_published` (MSK), `source`.

## Состав репозитория
- `sources.json` — список источников (URL листингов) и правила фильтрации ссылок.
- `scripts/aggregate.py` — парсер и сборщик ленты.
- `requirements.txt` — зависимости Python.
- `docs/index.html` — страница, рядом будет `docs/unified.json`.
- `.github/workflows/build.yml` — крон-воркфлоу, который строит ленту и коммитит результат.

---

## Шаг‑за‑шагом (с нуля)

### 1) Создайте репозиторий
1. Зарегистрируйтесь/войдите на GitHub.
2. Нажмите **New repository** → имя, например `normacs-feed` → Public → Create repository.
3. На странице репозитория нажмите **Add file → Upload files** и загрузите *все файлы* из архива.

### 2) Включите GitHub Pages
1. Зайдите: **Settings → Pages**.
2. **Source:** выберите **Deploy from a branch**.
3. **Branch:** `main` и **/docs**.
4. Сохраните. Через пару минут страница станет доступна по адресу, указанному на этой странице (что-то вроде `https://<логин>.github.io/<repo>/`). Лента будет по `.../unified.json`.

### 3) Разрешения для Actions (одноразово)
1. **Settings → Actions → General → Workflow permissions**.
2. Выберите **Read and write permissions** (это нужно, чтобы воркфлоу мог коммитить `docs/unified.json`). Сохраните.

### 4) Первый запуск
- Зайдите во вкладку **Actions** в репозитории.
- Откройте джоб **Build unified feed** → кнопка **Run workflow** (workflow_dispatch).
- По завершении проверьте вкладку **Code**: должен появиться `docs/unified.json`.
- Зайдите на вашу GitHub Pages: `https://<логин>.github.io/<repo>/unified.json` — там должна быть лента.

### 5) Ежечасные обновления
- Воркфлоу запланирован кроном `0 * * * *` (раз в час).
- Он бережно качает страницы с `If-Modified-Since/ETag`, делает паузы между запросами и обновляет ленту, только если есть изменения.

---

## Как это работает

1. `scripts/aggregate.py` читает `sources.json`.
2. Для каждого листинга:
   - качает HTML (условный GET, кэш заголовков в `.cache/state.json`),
   - парсит ссылки (`<a href=...>`) и **фильтрует** по `include_patterns`,
   - у заголовка ищет дату рядом (в тексте родителя/соседей) — поддержаны форматы `dd.mm.yyyy [hh:mm]` и `18 сентября 2025`,
   - нормализует дату в MSK, собирает карточку.
3. Строит JSON Feed → `docs/unified.json`.

> **Примечание:** Файлы кэша (`.cache/`) сохраняются между запусками через `actions/cache`. Это уменьшает трафик и ускоряет сбор.

---

## Частые вопросы

**Q: Некоторые страницы рендерятся скриптами (JS) и парсер не видит новости.**  
A: Для 1–2 таких источников добавьте точечные `include_patterns` или подключите промежуточный RSS-Bridge (можно бесплатный хостинг). В большинстве случаев достаточно уточнить `include_patterns` в `sources.json`.

**Q: Как добавить/удалить источник?**  
A: Откройте `sources.json` и добавьте/удалите блок с полями:
```json
{
  "name": "Название",
  "base_url": "https://example.com",
  "start_url": "https://example.com/news",
  "include_patterns": ["/news"],
  "link_min_text_len": 20
}
```
Сохраните. Воркфлоу запустится сам (триггер по `push`).

**Q: Где брать классификаторы?**  
A: Для задачи NEWS они проставляются **на этапе отбора карточек** и берутся *строго* из вашего `classifiers.csv`. На этапе агрегатора (эта лента) теги не требуются.

**Q: Как сузить окно времени?**  
A: Лента хранит все текущие записи листингов. При отборе вы используете своё окно `WINDOW_START ... now (MSK)` — как и раньше.

**Q: Дедупликация с normacs.info?**  
A: В этом проекте лента не знает, что уже опубликовано. Сверку с normacs.info сделаем в вашем рабочем процессе (например, я буду проверять заголовки при ежедневном отборе). При желании можно добавить сюда скрейп вашей страницы `/news` и исключать дубли — это 10–15 строк кода.

---

## Локальный запуск (по желанию)
```bash
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate
pip install -r requirements.txt
python scripts/aggregate.py
# результат будет в docs/unified.json
```

## Тонкая настройка
- Скорость: измените `SLEEP_BETWEEN_REQUESTS` в `aggregate.py` (по умолчанию 2 сек).
- Таймаут HTTP: `REQUEST_TIMEOUT` (по умолчанию 30 сек).
- Фильтры ссылок: `include_patterns` в `sources.json`.
- Минимальная длина текста ссылки: `link_min_text_len` (защита от "читать далее").

---

## Безопасность и бережность
- Пользуемся `If-Modified-Since/ETag`, кэшируем ответы, делаем паузы между обращениями.
- `User-Agent` явно указан как технический бот с контактной ссылкой.

Удачи! Если что-то пойдёт не так — присылайте логи из вкладки **Actions**, подсвечу и поправим.
